{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ac351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'singleLayerCluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mepic_clustering\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_clusters, get_cluster_pos\n",
      "File \u001b[0;32m/global/u2/d/danieltm/Side_Work/PowerWeek/ePIC_Clustering_2023/epic_clustering/utils/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mplotting_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mml_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhelperV2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/global/u2/d/danieltm/Side_Work/PowerWeek/ePIC_Clustering_2023/epic_clustering/utils/helperV2.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# from truthCluster import truthCluster\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# from superCluster import superCluster\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# from singleLayerCluster import singleLayerCluster\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# from multiDepthCluster import multiDepthCluster\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mepic_clustering\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m truthCluster, superCluster, singleLayerCluster, multiDepthCluster\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakeTruthClusters\u001b[39m(event):\n\u001b[1;32m     11\u001b[0m     truthClusters \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/global/u2/d/danieltm/Side_Work/PowerWeek/ePIC_Clustering_2023/epic_clustering/classes/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39meventContainer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexampleMLBasedCluster\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmultiDepthCluster\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msingleLayerCluster\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtruthCluster\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m/global/u2/d/danieltm/Side_Work/PowerWeek/ePIC_Clustering_2023/epic_clustering/classes/multiDepthCluster.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msingleLayerCluster\u001b[39;00m \u001b[39mimport\u001b[39;00m singleLayerCluster\n\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mmultiDepthCluster\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, event, SLC):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'singleLayerCluster'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics.cluster import entropy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from epic_clustering.utils import plot_clusters, get_cluster_pos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5479a06",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "Our plan is to load some data and visualize the various features that are included in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9437ed8",
   "metadata": {},
   "source": [
    "## 1. Load a CSV file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a7d927",
   "metadata": {},
   "source": [
    "First, we load one of the CSV files. The exact way you do this will differ between a local notebook and a Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e7f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/global/cfs/cdirs/m3443/data/PowerWeek\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m csv_files \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(input_dir, f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(input_dir) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "input_dir = \"/global/cfs/cdirs/m3443/data/PowerWeek\"\n",
    "csv_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(csv_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ed2a5c",
   "metadata": {},
   "source": [
    "## 2. Initial Inspection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b08a3a41",
   "metadata": {},
   "source": [
    "Let's examine the dataframe and see what is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91de9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c17dddfd",
   "metadata": {},
   "source": [
    "## 3. Visualize Distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06c6150c",
   "metadata": {},
   "source": [
    "It's usually very useful to understand the histogrammed distribution of the features in the dataset. This helps with feature engineering, and gaining some physical intuition about how to further visualize combinations of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e440369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9f97d45",
   "metadata": {},
   "source": [
    "We can make some initial observations about this data...\n",
    "- The (x,y) plane is bounded by X, and energy deposits are ...\n",
    "- \n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a6d846a",
   "metadata": {},
   "source": [
    "## 4. Visualize some events in 2D space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f34bae3",
   "metadata": {},
   "source": [
    "Let's build up a visualization in an iterative way, using the above observations. First, let's select an event that has an average number of hits and particles, using the above distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f628a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4668bd8d",
   "metadata": {},
   "source": [
    "Now, we understand the bounds of the (x,y) plane, so we can plot this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95446c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f60241f5",
   "metadata": {},
   "source": [
    "We also know that we have energy as a feature, and **it is a feature that is going to be used as a measure of importance in the scoring function**. So, let's use energy as an opacity for the points. We see that it has a long tail, so let's take the square root to more clearly see the clusters. (Try plotting the energy directly to see how sparse the clusters are!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a7c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d52380a",
   "metadata": {},
   "source": [
    "We see that there are not **so** many particles in each event, so we can color-code them without completely losing the thread of what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a32211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb69262",
   "metadata": {},
   "source": [
    "In the above Z-distribution, we clearly see that deposits are localized around each of the X layers in the z-direction. We can therefore visualize our hits in the (x,y) plane for each of the X layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87eb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff5e6c4",
   "metadata": {},
   "source": [
    "## 5. Visualize some events in 3D space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59dd87ac",
   "metadata": {},
   "source": [
    "In 2D, the clusters are clearly visible, and we may even be able to identify some of them by eye. This is a good clue about where to start with our initial clustering algorithm. Let's see if we can do better by visualizing in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd268085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27d883d",
   "metadata": {},
   "source": [
    "I don't know about you, but it's **harder** to visualize clusters in 3D. This is not just human error, but a real effect - the distance between clusters on a given layer is around the same as between layers. This means that if we naively apply a clustering algorithm, it will probably cluster by both particle and layer. This is okay, provided we then use the layer information to reconstruct the particles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2569452",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerWeek",
   "language": "python",
   "name": "powerweek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
